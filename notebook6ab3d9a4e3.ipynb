{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers peft datasets accelerate bitsandbytes tensorboard\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tensorboard --logdir output/lora_checkpoints/logs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scripts/prepare_dataset.py\n\nimport json\nimport os\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = \"taide/Llama-3.1-TAIDE-LX-8B-Chat\"\nDATA_DIR = \"data\"\nOUTPUT_DIR = \"dataset_cache\"\nMAX_LENGTH = 2048\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\ndef tokenize(example):\n    prompt = example[\"input\"]\n    answer = example[\"output\"]\n    full_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{answer}\"\n    tokens = tokenizer(full_prompt, truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\n# Load raw JSONL as DatasetDict\ndataset = DatasetDict({\n    split: load_dataset(\"json\", data_files={split: os.path.join(DATA_DIR, f\"scheduling_{split}.jsonl\")})[split]\n    for split in [\"train\", \"dev\", \"test\"]\n})\n\n# Tokenize\ntokenized = dataset.map(tokenize, remove_columns=[\"input\", \"output\"])\ntokenized.save_to_disk(OUTPUT_DIR)\n\nprint(f\"✅ 資料已儲存至 {OUTPUT_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scripts/train.py\n\nimport os\nimport torch\nfrom datasets import load_from_disk\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    Trainer\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom transformers.integrations import TensorBoardCallback\nfrom huggingface_hub import login\n\n# === 登入 Hugging Face（若 token 存在） ===\nhf_token = os.environ.get(\"HF_TOKEN\", None)\nif hf_token:\n    print(\"🔐 Logging into Hugging Face Hub...\")\n    login(token=hf_token)\nelse:\n    print(\"ℹ️ 未提供 Hugging Face Token，將不會使用 push_to_hub 功能\")\n\n# === 模型與路徑設定 ===\nmodel_name = \"taide/Llama-3.1-TAIDE-LX-8B-Chat\"\ndataset_path = \"dataset_cache\"\noutput_dir = \"output/lora_checkpoints\"\n\n# === LoRA 設定 ===\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# === 載入模型與 tokenizer ===\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\n# === 載入資料集 ===\ndataset = load_from_disk(dataset_path)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# === 訓練參數 ===\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    logging_steps=50,\n    save_steps=200,\n    eval_steps=200,\n    num_train_epochs=3,\n    warmup_steps=100,\n    learning_rate=2e-4,\n    logging_dir=f\"{output_dir}/logs\",\n    save_total_limit=2,\n    bf16=True,\n    report_to=\"tensorboard\",\n    push_to_hub=True if hf_token else False,   # 自動決定是否上傳\n    hub_model_id=\"你的帳號/你的repo名稱\",  # 可自訂 repo 名稱\n    hub_strategy=\"every_save\"\n)\n\n# === 訓練 ===\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"dev\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[TensorBoardCallback()]\n)\n\ntrainer.train()\ntrainer.save_model(output_dir)\nprint(f\"✅ 模型與 LoRA 參數已儲存至 {output_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scripts/inference.py\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nfrom peft import PeftModel\nimport gradio as gr\n\nBASE_MODEL = \"taide/Llama-3.1-TAIDE-LX-8B-Chat\"\nLORA_PATH = \"output/lora_checkpoints\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# 載入 tokenizer 和模型（含 LoRA）\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model, LORA_PATH)\nmodel.eval()\n\n# 推論函數\ndef generate_schedule(input_text):\n    prompt = f\"<|user|>\\n{input_text.strip()}\\n<|assistant|>\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.95,\n            repetition_penalty=1.2,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    result = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n    return result.strip()\n\n# Gradio UI\ndemo = gr.Interface(\n    fn=generate_schedule,\n    inputs=gr.Textbox(lines=10, placeholder=\"請輸入排班任務，例如：請安排一位擅長炸物且能上大夜班的員工...\", label=\"排班任務\"),\n    outputs=gr.Textbox(label=\"模型建議人員\"),\n    title=\"🧠 LLaMA-3 排班任務助手 (LoRA Fine-Tuned)\",\n    allow_flagging=\"never\"\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}