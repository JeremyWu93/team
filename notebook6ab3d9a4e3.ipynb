{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers peft datasets accelerate bitsandbytes tensorboard\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tensorboard --logdir output/lora_checkpoints/logs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scripts/prepare_dataset.py\n\nimport json\nimport os\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = \"taide/Llama-3.1-TAIDE-LX-8B-Chat\"\nDATA_DIR = \"data\"\nOUTPUT_DIR = \"dataset_cache\"\nMAX_LENGTH = 2048\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\ndef tokenize(example):\n    prompt = example[\"input\"]\n    answer = example[\"output\"]\n    full_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{answer}\"\n    tokens = tokenizer(full_prompt, truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\n# Load raw JSONL as DatasetDict\ndataset = DatasetDict({\n    split: load_dataset(\"json\", data_files={split: os.path.join(DATA_DIR, f\"scheduling_{split}.jsonl\")})[split]\n    for split in [\"train\", \"dev\", \"test\"]\n})\n\n# Tokenize\ntokenized = dataset.map(tokenize, remove_columns=[\"input\", \"output\"])\ntokenized.save_to_disk(OUTPUT_DIR)\n\nprint(f\"âœ… è³‡æ–™å·²å„²å­˜è‡³ {OUTPUT_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scripts/train.py\n\nimport os\nimport torch\nfrom datasets import load_from_disk\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    Trainer\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom transformers.integrations import TensorBoardCallback\nfrom huggingface_hub import login\n\n# === ç™»å…¥ Hugging Faceï¼ˆè‹¥ token å­˜åœ¨ï¼‰ ===\nhf_token = os.environ.get(\"HF_TOKEN\", None)\nif hf_token:\n    print(\"ğŸ” Logging into Hugging Face Hub...\")\n    login(token=hf_token)\nelse:\n    print(\"â„¹ï¸ æœªæä¾› Hugging Face Tokenï¼Œå°‡ä¸æœƒä½¿ç”¨ push_to_hub åŠŸèƒ½\")\n\n# === æ¨¡å‹èˆ‡è·¯å¾‘è¨­å®š ===\nmodel_name = \"taide/Llama-3.1-TAIDE-LX-8B-Chat\"\ndataset_path = \"dataset_cache\"\noutput_dir = \"output/lora_checkpoints\"\n\n# === LoRA è¨­å®š ===\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# === è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer ===\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\n# === è¼‰å…¥è³‡æ–™é›† ===\ndataset = load_from_disk(dataset_path)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# === è¨“ç·´åƒæ•¸ ===\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    logging_steps=50,\n    save_steps=200,\n    eval_steps=200,\n    num_train_epochs=3,\n    warmup_steps=100,\n    learning_rate=2e-4,\n    logging_dir=f\"{output_dir}/logs\",\n    save_total_limit=2,\n    bf16=True,\n    report_to=\"tensorboard\",\n    push_to_hub=True if hf_token else False,   # è‡ªå‹•æ±ºå®šæ˜¯å¦ä¸Šå‚³\n    hub_model_id=\"ä½ çš„å¸³è™Ÿ/ä½ çš„repoåç¨±\",  # å¯è‡ªè¨‚ repo åç¨±\n    hub_strategy=\"every_save\"\n)\n\n# === è¨“ç·´ ===\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"dev\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[TensorBoardCallback()]\n)\n\ntrainer.train()\ntrainer.save_model(output_dir)\nprint(f\"âœ… æ¨¡å‹èˆ‡ LoRA åƒæ•¸å·²å„²å­˜è‡³ {output_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scripts/inference.py\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nfrom peft import PeftModel\nimport gradio as gr\n\nBASE_MODEL = \"taide/Llama-3.1-TAIDE-LX-8B-Chat\"\nLORA_PATH = \"output/lora_checkpoints\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# è¼‰å…¥ tokenizer å’Œæ¨¡å‹ï¼ˆå« LoRAï¼‰\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model, LORA_PATH)\nmodel.eval()\n\n# æ¨è«–å‡½æ•¸\ndef generate_schedule(input_text):\n    prompt = f\"<|user|>\\n{input_text.strip()}\\n<|assistant|>\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.95,\n            repetition_penalty=1.2,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    result = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n    return result.strip()\n\n# Gradio UI\ndemo = gr.Interface(\n    fn=generate_schedule,\n    inputs=gr.Textbox(lines=10, placeholder=\"è«‹è¼¸å…¥æ’ç­ä»»å‹™ï¼Œä¾‹å¦‚ï¼šè«‹å®‰æ’ä¸€ä½æ“…é•·ç‚¸ç‰©ä¸”èƒ½ä¸Šå¤§å¤œç­çš„å“¡å·¥...\", label=\"æ’ç­ä»»å‹™\"),\n    outputs=gr.Textbox(label=\"æ¨¡å‹å»ºè­°äººå“¡\"),\n    title=\"ğŸ§  LLaMA-3 æ’ç­ä»»å‹™åŠ©æ‰‹ (LoRA Fine-Tuned)\",\n    allow_flagging=\"never\"\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}